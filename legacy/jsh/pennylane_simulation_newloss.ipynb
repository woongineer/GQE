{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPT, GPTConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from Pennylane.Circuit_generate import gate_generate, QuantumEmbedding\n",
    "from dataload import get_data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train, X_eval, y_train, y_eval  = get_data(name='iris')\n",
    "n,p = X_train.shape\n",
    "\n",
    "\"\"\"If use qubit matching, set num_qubits = p  otherwise, it is up to user's choice\"\"\"\n",
    "num_qubits = 4\n",
    "op_pool_size = p*4*num_qubits\n",
    "max_gate = 16           # senario legth\n",
    "\n",
    "# TensorDataset을 사용하여 데이터셋 생성\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int32).to(device)\n",
    "\n",
    "X_eval_tensor = torch.tensor(X_eval, dtype=torch.float32).to(device)\n",
    "y_eval_tensor = torch.tensor(y_eval, dtype=torch.int32).to(device)\n",
    "\n",
    "# TensorDataset을 사용하여 데이터셋 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# DataLoader로 배치 단위로 데이터 로드\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define quantum embedding circuit\n",
    "dev = qml.device('default.qubit', wires=num_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def quantum_circuit(inputs):\n",
    "    tokens = inputs[:17]\n",
    "    data = inputs[17:]\n",
    "    QuantumEmbedding(tokens, n_qubits = num_qubits, data = data, matching=False)\n",
    "    return qml.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer model\n",
    "class Transformer(GPT):\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, n_sequences, max_new_tokens, temperature=1., device=\"cpu\"):\n",
    "        idx = torch.zeros(size=(n_sequences, 1), dtype=int, device=device)\n",
    "        total_logits = torch.zeros(size=(n_sequences, 1), device=device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits = self(idx_cond)\n",
    "            # pluck the logits at the final step\n",
    "            logits = logits[:, -1, :]\n",
    "            # set the logit of the first token so that its probability will be zero\n",
    "            logits[:, 0] = -float(\"inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities and scale by desired temperature\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # # Accumulate logits\n",
    "            total_logits += torch.gather(logits, index=idx_next, dim=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx, total_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hybrid network\n",
    "class GNQE(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, dropout, n_layer, n_head, n_embd, bias):\n",
    "        super(GNQE, self).__init__()\n",
    "        self.transformer_decoder = Transformer((GPTConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            block_size=block_size,\n",
    "            dropout=dropout,\n",
    "            n_layer = n_layer,\n",
    "            n_head = n_head,\n",
    "            n_embd = n_embd,\n",
    "            bias=False)\n",
    "            ))\n",
    "        \n",
    "        self.quantum_layer = qml.qnn.TorchLayer(quantum_circuit, \n",
    "                                                weight_shapes={})\n",
    "    \n",
    "    def gpt_forward(self, idx):\n",
    "        logit = self.transformer_decoder.forward(idx)\n",
    "        return logit\n",
    "\n",
    "    def token_generate(self, max_new_tokens, temperature, device):\n",
    "        tokens, logit = self.transformer_decoder.generate(n_sequences = 1, max_new_tokens = max_new_tokens,temperature=temperature, device=device)\n",
    "        return tokens, logit\n",
    "    \n",
    "    def Qembedding(self, tokens, data, device):\n",
    "        state = self.quantum_layer(torch.cat((tokens, data), dim=0).to(device))\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.38M\n"
     ]
    }
   ],
   "source": [
    "model = GNQE(\n",
    "    vocab_size = op_pool_size + 1,\n",
    "    block_size = max_gate,\n",
    "    dropout = 0.2,\n",
    "    n_layer = 6,\n",
    "    n_head = 6,\n",
    "    n_embd = 216,\n",
    "    bias=False\n",
    ").to('cuda')\n",
    "TEMPERATURE = 10\n",
    "r = 0.99\n",
    "opt = torch.optim.AdamW(model.parameters())\n",
    "n_epochs = 1000\n",
    "loss_history = []\n",
    "train_fidloss_history = []\n",
    "ev_fidloss_history = []\n",
    "ev_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5,  loss : 0.16705073416233063, fidloss : -0.19465282559394836\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 10,  loss : 0.32358479499816895, fidloss : -0.5599483847618103\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 15,  loss : 0.017713651061058044, fidloss : -0.5760005712509155\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 20,  loss : 0.5995155572891235, fidloss : -0.20577333867549896\n",
      "Evaluation loss : 1.5612685680389404\n",
      "Evaluation fidloss : 0.22274932265281677\n",
      "Epoch : 25,  loss : 0.687972366809845, fidloss : -0.18699832260608673\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 30,  loss : 0.42819419503211975, fidloss : -0.424085408449173\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 35,  loss : 0.642946720123291, fidloss : -0.2208465039730072\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 40,  loss : 0.38564157485961914, fidloss : -0.4759393632411957\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 45,  loss : 0.38982537388801575, fidloss : -0.45957717299461365\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 50,  loss : 0.4708041250705719, fidloss : -0.37665608525276184\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 55,  loss : 0.3540257513523102, fidloss : -0.5189505219459534\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 60,  loss : 0.5340057611465454, fidloss : -0.3136647939682007\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 65,  loss : 0.19524915516376495, fidloss : -0.8167392015457153\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 70,  loss : 0.34478017687797546, fidloss : -0.532412588596344\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 75,  loss : 0.532264769077301, fidloss : -0.3153071105480194\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 80,  loss : 0.36309728026390076, fidloss : -0.5065418481826782\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 85,  loss : 0.3905525803565979, fidloss : -0.4700957238674164\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 90,  loss : 0.41488662362098694, fidloss : -0.4398748278617859\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 95,  loss : 0.26477351784706116, fidloss : -0.6644403338432312\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 100,  loss : 0.3462004065513611, fidloss : -0.5303681492805481\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 105,  loss : 0.3697589933872223, fidloss : -0.49745187163352966\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 110,  loss : 0.5235211849212646, fidloss : -0.32358241081237793\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 115,  loss : 0.5280474424362183, fidloss : -0.319284051656723\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 120,  loss : 0.8637644648551941, fidloss : -0.07322753965854645\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 125,  loss : 0.15510453283786774, fidloss : -0.9318251013755798\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 130,  loss : 0.25182053446769714, fidloss : -0.689518928527832\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 135,  loss : 0.4978216886520386, fidloss : -0.3487545847892761\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 140,  loss : 0.4838278889656067, fidloss : -0.3630128800868988\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 145,  loss : 0.521867573261261, fidloss : -0.32517069578170776\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 150,  loss : 0.6992234587669373, fidloss : -0.17889150977134705\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 155,  loss : 0.41716358065605164, fidloss : -0.43681713938713074\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 160,  loss : 0.36881864070892334, fidloss : -0.49872416257858276\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 165,  loss : 0.735450804233551, fidloss : -0.1536346971988678\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 170,  loss : 0.9217084646224976, fidloss : -0.04076307266950607\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 175,  loss : 0.2579385042190552, fidloss : -0.6775169968605042\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 180,  loss : 0.2878916561603546, fidloss : -0.6225811243057251\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 185,  loss : 0.41607463359832764, fidloss : -0.4384453594684601\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 190,  loss : 0.667934238910675, fidloss : -0.2017827183008194\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 195,  loss : 0.7192248702049255, fidloss : -0.16479063034057617\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 200,  loss : 0.2449086308479309, fidloss : -0.7034350633621216\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 205,  loss : 0.7224196791648865, fidloss : -0.16257449984550476\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 210,  loss : 0.4282519221305847, fidloss : -0.4240218997001648\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 215,  loss : 0.3724815249443054, fidloss : -0.49378398060798645\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 220,  loss : 0.23931202292442322, fidloss : -0.714993417263031\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 225,  loss : 0.6142833232879639, fidloss : -0.2436494678258896\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 230,  loss : 0.7336980104446411, fidloss : -0.15482893586158752\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 235,  loss : 0.6855111718177795, fidloss : -0.1887952983379364\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 240,  loss : 0.23884446918964386, fidloss : -0.7159713506698608\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 245,  loss : 0.49180376529693604, fidloss : -0.3548377752304077\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 250,  loss : 0.4943343997001648, fidloss : -0.35227149724960327\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 255,  loss : 0.491356760263443, fidloss : -0.3552923798561096\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 260,  loss : 0.7705385684967041, fidloss : -0.13033273816108704\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 265,  loss : 0.5325338244438171, fidloss : -0.31505441665649414\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 270,  loss : 0.28724414110183716, fidloss : -0.6237114667892456\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 275,  loss : 0.5446827411651611, fidloss : -0.3037759065628052\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 280,  loss : 0.37246689200401306, fidloss : -0.49380362033843994\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 285,  loss : 0.3471650779247284, fidloss : -0.5289774537086487\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 290,  loss : 0.26292115449905396, fidloss : -0.6679505109786987\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 295,  loss : 0.2646775245666504, fidloss : -0.6646215915679932\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 300,  loss : 0.6106645464897156, fidloss : -0.24660372734069824\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 305,  loss : 0.3148037791252136, fidloss : -0.5779029130935669\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 310,  loss : 0.33789509534835815, fidloss : -0.542509913444519\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 315,  loss : 0.45501312613487244, fidloss : -0.3937145471572876\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 320,  loss : 0.36175188422203064, fidloss : -0.5083984136581421\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 325,  loss : 0.2663823962211609, fidloss : -0.6614111661911011\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 330,  loss : 0.949093759059906, fidloss : -0.02612381801009178\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 335,  loss : 0.31671029329299927, fidloss : -0.5748838782310486\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 340,  loss : 0.5528295636177063, fidloss : -0.2963527739048004\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 345,  loss : 0.47273117303848267, fidloss : -0.37461426854133606\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 350,  loss : 0.5048993825912476, fidloss : -0.3416981101036072\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 355,  loss : 0.7934715151786804, fidloss : -0.11566881835460663\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 360,  loss : 0.16173893213272095, fidloss : -0.9108858704566956\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 365,  loss : 0.41063618659973145, fidloss : -0.44502389430999756\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 370,  loss : 0.8627397418022156, fidloss : -0.0738210454583168\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 375,  loss : 1.7925328016281128, fidloss : 0.2918148636817932\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 380,  loss : 0.056256745010614395, fidloss : -1.4389146566390991\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 385,  loss : 0.6448683142662048, fidloss : -0.2193545550107956\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 390,  loss : 1.2441487312316895, fidloss : 0.10922577977180481\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 395,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 400,  loss : 1.1859456300735474, fidloss : 0.08527025580406189\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 405,  loss : 0.2659444510936737, fidloss : -0.6622339487075806\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 410,  loss : 0.06597339361906052, fidloss : -1.359251856803894\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 415,  loss : 0.8332818746566772, fidloss : -0.09119163453578949\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 420,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 425,  loss : 0.4156673550605774, fidloss : -0.4389350116252899\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 430,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 435,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 440,  loss : 0.024074425920844078, fidloss : -1.8633025884628296\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 445,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 450,  loss : 0.1353352665901184, fidloss : -1.0000001192092896\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 455,  loss : 1.8459566831588745, fidloss : 0.30649882555007935\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 460,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 465,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 470,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 475,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 480,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 485,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 490,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 495,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 500,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 505,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 510,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 515,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 520,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 525,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 530,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 535,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 540,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 545,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 550,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 555,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 560,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 565,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 570,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 575,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 580,  loss : 0.1353352665901184, fidloss : -1.0000001192092896\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 585,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 590,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 595,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 600,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 605,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 610,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 615,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 620,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 625,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 630,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 635,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 640,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 645,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 650,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 655,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 660,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 665,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 670,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 675,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 680,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 685,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 690,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 695,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 700,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 705,  loss : 0.030197378247976303, fidloss : -1.7500001192092896\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 710,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 715,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 720,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 725,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 730,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 735,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 740,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 745,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 750,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 755,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 760,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 765,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 770,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 775,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 780,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 785,  loss : 0.1353352665901184, fidloss : -1.0000001192092896\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 790,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 795,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 800,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 805,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 810,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 815,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 820,  loss : 0.030197378247976303, fidloss : -1.7500001192092896\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 825,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 830,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 835,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 840,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 845,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 850,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 855,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 860,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 865,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 870,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 875,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 880,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 885,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 890,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 895,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 900,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 905,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 910,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 915,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 920,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 925,  loss : 0.43459823727607727, fidloss : -0.4166666865348816\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 930,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 935,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 940,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 945,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 950,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 955,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 960,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 965,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 970,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 975,  loss : 1.0, fidloss : -0.0\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 980,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 985,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 990,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 995,  loss : 1.6487213373184204, fidloss : 0.2500000298023224\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n",
      "Epoch : 1000,  loss : 1.9477338790893555, fidloss : 0.3333333432674408\n",
      "Evaluation loss : 1.6926844120025635\n",
      "Evaluation fidloss : 0.2631579041481018\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # 학습 루프 내에서 손실 계산\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        batch_size = X_batch.size(0)\n",
    "\n",
    "        # 배치의 상태 벡터 계산\n",
    "        states = []\n",
    "        token, logit = model.token_generate(max_new_tokens = max_gate, temperature = TEMPERATURE, device = device)\n",
    "        for i in range(batch_size):\n",
    "            state = model.Qembedding(token.squeeze(), X_batch[i], device = device)\n",
    "            states.append(state)\n",
    "        states = torch.stack(states)  # (batch_size, state_vector_size)\n",
    "\n",
    "        # 상태 벡터 정규화\n",
    "        states = states / torch.norm(states, dim=1, keepdim=True)\n",
    "\n",
    "        # 상태 벡터의 켤레 복소수\n",
    "        states_conj = torch.conj(states)\n",
    "\n",
    "        # 내적 행렬 계산\n",
    "        inner_products = torch.matmul(states_conj, states.T)  # (batch_size, batch_size)\n",
    "\n",
    "        # 피델리티 행렬 계산\n",
    "        fidelity_matrix = torch.abs(inner_products) ** 2\n",
    "\n",
    "        # 라벨 곱 행렬 계산\n",
    "        labels = y_batch.view(-1)  # (batch_size,)\n",
    "        label_products = torch.outer(labels, labels).to(device)  # (batch_size, batch_size)\n",
    "\n",
    "        # 손실 행렬 계산\n",
    "        loss_matrix = torch.mul(label_products, fidelity_matrix).to(device)\n",
    "\n",
    "        # 상삼각 행렬에서 i < j인 요소들 선택\n",
    "        indices = torch.triu_indices(batch_size, batch_size, offset=1)\n",
    "        loss_values = loss_matrix[indices[0], indices[1]]\n",
    "        loss_values = -5*torch.mean(loss_values)\n",
    "\n",
    "        # 총 손실 계산\n",
    "        loss = (torch.exp(loss_values) - torch.exp(-logit.squeeze()))**2\n",
    "        train_fidloss_history.append(loss_values.item())\n",
    "        loss_history.append(loss.item())\n",
    "        # 역전파 및 옵티마이저 스텝\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    TEMPERATURE = TEMPERATURE * r\n",
    "\n",
    "    if (epoch+1) % 5 == 0 :\n",
    "        model.eval()\n",
    "        ev_size = X_eval_tensor.size(0)\n",
    "        ev_states = []\n",
    "        ev_token, ev_logit = model.token_generate(max_new_tokens=max_gate,temperature=.001 ,device = device)\n",
    "        for i in range(ev_size):\n",
    "            ev_state = model.Qembedding(ev_token.squeeze(), X_eval_tensor[i], device = device)\n",
    "            ev_states.append(ev_state)\n",
    "        ev_states = torch.stack(ev_states)  \n",
    "        ev_states = ev_states / torch.norm(ev_states, dim=1, keepdim=True)\n",
    "        ev_states_conj = torch.conj(ev_states)\n",
    "        ev_inner_products = torch.matmul(ev_states_conj, ev_states.T) \n",
    "        ev_fidelity_matrix = (torch.abs(ev_inner_products) ** 2).to(device)\n",
    "        ev_labels = y_eval_tensor.view(-1) \n",
    "        ev_label_products = torch.outer(ev_labels, ev_labels)\n",
    "        ev_loss_matrix = torch.mul(ev_label_products, ev_fidelity_matrix).to(device)\n",
    "        ev_indices = torch.triu_indices(ev_size, ev_size, offset=1)\n",
    "        ev_loss_values = ev_loss_matrix[ev_indices[0], ev_indices[1]]\n",
    "        ev_loss_values = -5*torch.mean(ev_loss_values)\n",
    "        ev_loss = (torch.exp(ev_loss_values) - torch.exp(-ev_logit.squeeze()))**2\n",
    "        \n",
    "        ev_fidloss_history.append(ev_loss_values.item())\n",
    "        ev_loss_history.append(ev_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Epoch : {epoch+1},  loss : {loss}, fidloss : {loss_values}\")\n",
    "        print(f\"Evaluation loss : {ev_loss}\")\n",
    "        print(f\"Evaluation fidloss : {ev_loss_values}\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9477, device='cuda:0', grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.exp(loss_values)-torch.exp(-logit.squeeze()))**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
